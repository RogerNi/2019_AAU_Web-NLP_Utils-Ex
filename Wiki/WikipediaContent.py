'''
@Author: NI Roger
@Date: 2019-05-29 10:55:13
@LastEditors: NI Roger
@LastEditTime: 2019-06-11 13:07:19
@Description: Get Wikipedia Content for any keyword
'''


import wikipedia as wk
import pickle
import json
import GoogleRedirect as gred

def getContentForOneEntry(entryName):
    page = wk.page(entryName)
    return page.content

def getContentsForList(entriesList):
    """Get contents for a list of keyword

    Will first try to find an alternative name for the keyword. Not found, the original keyword will be used.

    Arg:
        entriesList: a list containing all the keyword

    Returns:
        1:  A dictionary containing all the crawling results with keywords as keys and contents as values
        2:  A list containing all the warning messages
        3:  The keywords that cannot be found
    """
    contents = {}
    warnings = []
    excludeList = []
    for entry in entriesList:
        if entry in contents:
            warnings.append("[Duplicate] "+entry+" already exists!")
        else:
            # try:
            #     contents[entry] = getContentForOneEntry(entry)
            # except Exception as e:
            #     warnings.append("[Wiki_Error] While crawling entry \"" + entry +"\", exceptions occur: " + e)
            try:
                contents[entry], alterName = tryUsingAlterName(entry)
                warnings.append("[Wiki] Alternative name found by Google. " + entry +" replaced by " + alterName)
            except Exception as e:
                warnings.append("[Wiki_Error] NO alternative name found by Google: " + entry)
                try:
                    contents[entry] = getContentForOneEntry(entry)
                except Exception as e:
                    warnings.append("[Wiki_Error] While crawling entry \"" + entry +"\", exceptions occur: ")
                    excludeList.append(entry)
    return (contents,warnings,excludeList)

def writeToFile(contents, filePath):
    """Write Contents to file

    Args:
        contents: contents generated by getContentsForList function
        filePath: the path to write file  
    """
    pickle_out = open(filePath,'w',encoding="UTF-8")
    json.dump(contents,pickle_out)
    pickle_out.close()

def crawlAndWrite(entriesList, filePath):
    """Call getContentsForList function and write results to file

    Call getContentsForList function and then process warnings and errors.
    Finally, write the crawling results to file.

    Args:
        entriesList: a list containing all the keyword
        filePath: the path to write file
    """
    contents, warnings, exclude = getContentsForList(entriesList)
    if warnings:
        print("==============Warnings=============")
        for warn in warnings:
            print(warn)
        print()
    if exclude:
        print("=========Entries Not Found=========")
        for ex in exclude:
            print(ex)
        print()
    writeToFile(contents, filePath)
    print("Write to File Done!")
    
def tryUsingAlterName(entry):
    """Get alternative name from Google

    GoogleRedirect package used

    Arg:
        entry: keyword

    Return:
        1:  crawling result using alternative name
        2: alternative name
    """
    name = gred.getWikiKeyWordFromGoogle(entry)
    if not name:
        raise Exception
    return (getContentForOneEntry(name),name)
